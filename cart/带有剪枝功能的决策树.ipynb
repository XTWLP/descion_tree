{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  一、决策树的构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 节点定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义节点\n",
    "class Node(object): \n",
    "    '''\n",
    "    definition of decision node class\n",
    "    \n",
    "    attr: attribution as parent for a new branching \n",
    "    attr_down: dict: {key, value}\n",
    "            key:   categoric:  categoric attr_value \n",
    "            \n",
    "        \n",
    "                   continuous: '<= div_value' for small part\n",
    "                               '> div_value' for big part\n",
    "            value: children (Node class)\n",
    "    label： class label (the majority of current sample labels)\n",
    "    ''' \n",
    "    def __init__(self, attr_init=None, label_init=None, attr_down_init={} ):  \n",
    "        self.attr = attr_init  \n",
    "        self.label = label_init \n",
    "        self.attr_down = attr_down_init "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.树的构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）建树中用到的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算样本的类的数量（好瓜/坏瓜），及其计数\n",
    "def NodeLabel(label_arr):\n",
    "    '''\n",
    "    calculating the appeared label and it's counts\n",
    "     \n",
    "    @param label_arr: data array for class labels\n",
    "    @return label_count: dict, the appeared label and it's counts\n",
    "    '''  \n",
    "    label_count = {}       # store count of label \n",
    "      \n",
    "    for label in label_arr:\n",
    "        if label in label_count: label_count[label] += 1\n",
    "        else: label_count[label] = 1\n",
    "        \n",
    "    return label_count\n",
    "#得到某个属性的所有取值和取到该值的样本数量\n",
    "def ValueCount(data_arr):\n",
    "    '''\n",
    "    calculating the appeared value for categoric attribute and it's counts\n",
    "    \n",
    "    @param data_arr: data array for an attribute\n",
    "    @return value_count: dict, the appeared value and it's counts\n",
    "    '''\n",
    "    value_count = {}       # store count of value \n",
    "      \n",
    "    for label in data_arr:\n",
    "        if label in value_count: value_count[label] += 1\n",
    "        else: value_count[label] = 1\n",
    "        \n",
    "    return value_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）树的构建及其功能函数（预测，计算精度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建树的函数（训练函数）\n",
    "import pandas as pd\n",
    "data_file_encode = \"gb18030\"\n",
    "with open(\"watermelon_2.csv\", mode = 'r', encoding = data_file_encode) as data_file:\n",
    "    data = pd.read_csv(data_file)\n",
    "def TreeGenerate(df):\n",
    "    ''' \n",
    "    Branching for decision tree using recursion \n",
    "     \n",
    "    @param df: the pandas dataframe of the data_set\n",
    "    @return root: Node, the root node of decision tree\n",
    "    '''  \n",
    "    # generating a new root node\n",
    "    new_node = Node(None, None, {})\n",
    "    label_arr = df[df.columns[-1]]\n",
    "    \n",
    "    label_count = NodeLabel(label_arr)\n",
    "    if label_count:  # assert the label_count isn's empty\n",
    "        new_node.label= max(label_count, key=label_count.get) \n",
    "            \n",
    "        # end if there is only 1 class in current node data\n",
    "        # end if attribution array is empty\n",
    "        if len(label_count) == 1 or len(label_arr) == 0:\n",
    "            return new_node\n",
    "        \n",
    "        # get the optimal attribution for a new branching\n",
    "        new_node.attr, div_value = OptAttr_Ent(df)  # via Gini index \n",
    "        \n",
    "        # recursion\n",
    "        if div_value == 0:  # categoric variable\n",
    "            value_count = ValueCount(data[new_node.attr]) \n",
    "            for value in value_count:\n",
    "                df_v = df[ df[new_node.attr].isin([value]) ]  # get sub set\n",
    "                # delete current attribution\n",
    "                df_v = df_v.drop(new_node.attr, 1)  \n",
    "                if len(df_v) == 0:\n",
    "                    new_node.attr_down[value] = Node(None,new_node.label, {})\n",
    "                    return new_node\n",
    "                else:\n",
    "                    new_node.attr_down[value] = TreeGenerate(df_v)     #迭代创立下一级节点\n",
    "                \n",
    "        else:  # continuous variable # left and right child\n",
    "            value_l = \"<=%.3f\" % div_value\n",
    "            value_r = \">%.3f\" % div_value\n",
    "            df_v_l = df[ df[new_node.attr] <= div_value ]  # get sub set\n",
    "            df_v_r = df[ df[new_node.attr] > div_value ]\n",
    " \n",
    "            new_node.attr_down[value_l] = TreeGenerate(df_v_l)\n",
    "            new_node.attr_down[value_r] = TreeGenerate(df_v_r)\n",
    "        \n",
    "    return new_node\n",
    "\n",
    "\n",
    "#预测函数\n",
    "def Predict(root, df_sample):   \n",
    "    '''\n",
    "    make a predict based on root\n",
    "    \n",
    "    @param root: Node, root Node of the decision tree\n",
    "    @param df_sample: dataframe, a sample line \n",
    "    '''\n",
    "    try :\n",
    "        import re # using Regular Expression to get the number in string\n",
    "    except ImportError :\n",
    "        print(\"module re not found\")\n",
    "    \n",
    "    while root.attr != None :        \n",
    "        # continuous variable\n",
    "        if df_sample[root.attr].dtype == float:\n",
    "            # get the div_value from root.attr_down\n",
    "            for key in list(root.attr_down):\n",
    "                num = re.findall(r\"\\d+\\.?\\d*\",key)\n",
    "                div_value = float(num[0])\n",
    "                break\n",
    "            if df_sample[root.attr].values[0] <= div_value:\n",
    "                key = \"<=%.3f\" % div_value\n",
    "                root = root.attr_down[key]\n",
    "            else:\n",
    "                key = \">%.3f\" % div_value\n",
    "                root = root.attr_down[key]\n",
    "                \n",
    "        # categoric variable\n",
    "        else:  \n",
    "            key = df_sample[root.attr].values[0]\n",
    "            # check whether the attr_value in the child branch\n",
    "            if key in root.attr_down: \n",
    "                root = root.attr_down[key]\n",
    "            else: \n",
    "                break\n",
    "            \n",
    "    return root.label \n",
    "\n",
    "\n",
    "\n",
    "#计算预测精确度的函数\n",
    "def PredictAccuracy(root, df_test):  \n",
    "    '''\n",
    "    calculating accuracy of prediction on test set\n",
    "    \n",
    "    @param root: Node, root Node of the decision tree\n",
    "    @param df_test: dataframe, test data set\n",
    "    @return accuracy, float,\n",
    "    '''\n",
    "    if len(df_test.index) == 0: return 0\n",
    "    pred_true = 0\n",
    "    for i in df_test.index:\n",
    "        label = Predict(root, df_test[df_test.index == i])\n",
    "        if label == df_test[df_test.columns[-1]][i]:\n",
    "            pred_true += 1\n",
    "    return pred_true / len(df_test.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）信息增益和信息熵的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "optimal attribution selection in ID3 algorithm based on information entropy\n",
    "'''\n",
    "#挑选信息增益最大的属性\n",
    "def OptAttr_Ent(df):\n",
    "    '''\n",
    "    find the optimal attributes of current data_set based on info entropy\n",
    "     \n",
    "    @param df: the pandas dataframe of the data_set \n",
    "    @return opt_attr:  the optimal attribution for branch\n",
    "    @return div_value: for discrete variable value = 0\n",
    "                       for continuous variable value = t for bisection divide value\n",
    "    '''  \n",
    "    info_gain = 0\n",
    "    \n",
    "    for attr_id in df.columns[1:-1]:\n",
    "        info_gian_tmp, div_value_tmp = InfoGain(df, attr_id)\n",
    "        if  info_gian_tmp > info_gain : \n",
    "            info_gain = info_gian_tmp\n",
    "            opt_attr = attr_id\n",
    "            div_value = div_value_tmp\n",
    "        \n",
    "    return opt_attr, div_value\n",
    "\n",
    "\n",
    "#计算属性的信息增益\n",
    "def InfoGain(df, attr_id):\n",
    "    '''\n",
    "    calculating the information gain of an attribution\n",
    "     \n",
    "    @param df:      dataframe, the pandas dataframe of the data_set\n",
    "    @param attr_id: the target attribution in df\n",
    "    @return info_gain: the information gain of current attribution\n",
    "    @return div_value: for discrete variable, value = 0\n",
    "                   for continuous variable, value = t (the division value)\n",
    "    '''  \n",
    "    info_gain = InfoEnt(df.values[:,-1])  # info_gain for the whole label\n",
    "    div_value = 0  # div_value for continuous attribute\n",
    "    \n",
    "    n = len(df[attr_id])  # the number of sample\n",
    "    # 1.for continuous variable using method of bisection\n",
    "    if df[attr_id].dtype == float:\n",
    "        sub_info_ent = {}  # store the div_value (div) and it's subset entropy\n",
    "        \n",
    "        df = df.sort_values([attr_id], ascending=1)  # sorting via column\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        data_arr = df[attr_id]\n",
    "        label_arr = df[df.columns[-1]]\n",
    "        \n",
    "        for i in range(n-1):\n",
    "            div = (data_arr[i] + data_arr[i+1]) / 2\n",
    "            sub_info_ent[div] = ( (i+1) * InfoEnt(label_arr[0:i+1]) / n ) \\\n",
    "                              + ( (n-i-1) * InfoEnt(label_arr[i+1:-1]) / n )\n",
    "        # our goal is to get the min subset entropy sum and it's divide value\n",
    "        div_value, sub_info_ent_max = min(sub_info_ent.items(), key=lambda x: x[1])\n",
    "        info_gain -= sub_info_ent_max\n",
    "        \n",
    "    # 2.for discrete variable (categoric variable)\n",
    "    else:\n",
    "        data_arr = df[attr_id]\n",
    "        label_arr = df[df.columns[-1]]\n",
    "        value_count = ValueCount(data_arr)\n",
    "            \n",
    "        for key in value_count:\n",
    "            key_label_arr = label_arr[data_arr == key]\n",
    "            info_gain -= value_count[key] * InfoEnt(key_label_arr) / n\n",
    "    \n",
    "    return info_gain, div_value\n",
    "\n",
    "\n",
    "#计算属性的信息熵\n",
    "def InfoEnt(label_arr):\n",
    "    '''\n",
    "    calculating the information entropy of an attribution\n",
    "     \n",
    "    @param label_arr: ndarray, class label array of data_arr\n",
    "    @return ent: the information entropy of current attribution\n",
    "    ''' \n",
    "    try :\n",
    "        from math import log2\n",
    "    except ImportError :\n",
    "        print(\"module math.log2 not found\")\n",
    "    \n",
    "    ent = 0\n",
    "    n = len(label_arr)\n",
    "    label_count = NodeLabel(label_arr)\n",
    "    \n",
    "    for key in label_count:\n",
    "        ent -= ( label_count[key] / n ) * log2( label_count[key] / n )\n",
    "    \n",
    "    return ent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （4）基尼指数和基尼值的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#挑选基尼指数最小的属性\n",
    "def OptAttr_Gini(df):\n",
    "    '''\n",
    "    find the optimal attributes of current data_set based on gini index\n",
    "     \n",
    "    @param df: the pandas dataframe of the data_set \n",
    "    @return opt_attr:  the optimal attribution for branch\n",
    "    @return div_value: for discrete variable value = 0\n",
    "                       for continuous variable value = t for bisection divide value\n",
    "    '''\n",
    "    gini_index = float('Inf')      \n",
    "    for attr_id in df.columns[1:-1]:                                #有几个属性就循环几次\n",
    "        gini_index_tmp, div_value_tmp = GiniIndex(df, attr_id)        #计算基尼值 和连续变量的划分值\n",
    "        if  gini_index_tmp < gini_index :                           # 比较并找到最小的基尼指数所对应的属性\n",
    "            gini_index = gini_index_tmp\n",
    "            opt_attr = attr_id\n",
    "            div_value = div_value_tmp    \n",
    "        \n",
    "    return opt_attr, div_value\n",
    "\n",
    "\n",
    "#计算基尼指数\n",
    "def GiniIndex(df, attr_id):\n",
    "    '''\n",
    "    calculating the gini index of an attribution\n",
    "     \n",
    "    @param df:      dataframe, the pandas dataframe of the data_set\n",
    "    @param attr_id: the target attribution in df\n",
    "    @return gini_index: the gini index of current attribution\n",
    "    @return div_value: for discrete variable, value = 0\n",
    "                   for continuous variable, value = t (the division value)\n",
    "    '''  \n",
    "    gini_index = 0  # info_gain for the whole label\n",
    "    div_value = 0  # 连续变量的划分值\n",
    "    \n",
    "    n = len(df[attr_id])  # 样本数量\n",
    "    \n",
    "    # 1.对于连续变量，使用分切的方法\n",
    "    if df[attr_id].dtype == float:\n",
    "        sub_gini = {}  # 存储div_value（div）和它的子集gini值\n",
    "        \n",
    "        df = df.sort_values([attr_id], ascending=1)  # sorting via column\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        data_arr = df[attr_id]\n",
    "        label_arr = df[df.columns[-1]]\n",
    "        \n",
    "        for i in range(n-1):\n",
    "            div = (data_arr[i] + data_arr[i+1]) / 2\n",
    "            sub_gini[div] = ( (i+1) * Gini(label_arr[0:i+1]) / n ) \\\n",
    "                              + ( (n-i-1) * Gini(label_arr[i+1:-1]) / n )\n",
    "        # 我们的目标是得到最小子集熵的总和和它对应的划分值\n",
    "        div_value, gini_index = min(sub_gini.items(), key=lambda x: x[1])\n",
    "        \n",
    "    # 2.对离散变量（分类变量）而言，公式计算基尼指数。\n",
    "    else:\n",
    "        data_arr = df[attr_id]\n",
    "        label_arr = df[df.columns[-1]]\n",
    "        value_count = ValueCount(data_arr)\n",
    "            \n",
    "        for key in value_count:                          \n",
    "            key_label_arr = label_arr[data_arr == key]\n",
    "            gini_index += value_count[key] * Gini(key_label_arr) / n    \n",
    "    \n",
    "    return gini_index, div_value\n",
    "\n",
    "\n",
    "#计算该属性的基尼值\n",
    "def Gini(label_arr):\n",
    "\n",
    "    '''\n",
    "    calculating the gini value of an attribution \n",
    "    @param label_arr: ndarray, class label array of data_arr\n",
    "    @return gini: the information entropy of current attribution\n",
    "    '''     \n",
    "    gini = 1\n",
    "    \n",
    "    n = len(label_arr)\n",
    "    label_count = NodeLabel(label_arr)      #该属性共有几种取值\n",
    "    for key in label_count:\n",
    "        gini -= ( label_count[key] / n ) * ( label_count[key] / n )\n",
    "    \n",
    "    return gini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （5）带预剪枝的树的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预剪枝函数\n",
    "import pandas as pd\n",
    "\n",
    "data_file_encode = \"gb18030\"\n",
    "with open(\"watermelon_2.csv\", mode='r', encoding=data_file_encode) as data_file:\n",
    "    data = pd.read_csv(data_file)\n",
    "\n",
    "\n",
    "def PrePurn(df_train, df_test, a0 = 0):\n",
    "    '''\n",
    "    pre-purning to generating a decision tree\n",
    "\n",
    "    @param df_train: dataframe, the training set to generating a tree\n",
    "    @param df_test: dataframe, the testing set for purning decision\n",
    "    @return root: Node, root of the tree using purning\n",
    "    '''\n",
    "    # generating a new root node\n",
    "    new_node = Node(None, None, {})\n",
    "    label_arr = df_train[df_train.columns[-1]]\n",
    "\n",
    "    label_count = NodeLabel(label_arr)\n",
    "    if label_count:  # assert the label_count isn's empty\n",
    "        new_node.label = max(label_count, key=label_count.get)\n",
    "\n",
    "        # end if there is only 1 class in current node data\n",
    "        # end if attribution array is empty\n",
    "        if len(label_count) == 1 or len(label_arr) == 0:\n",
    "            return new_node\n",
    "\n",
    "        # calculating the test accuracy up to current node\n",
    "        if a0==0:\n",
    "            a0 = PredictAccuracy(new_node, df_test)  # 在测试集test上计算不进行分支的模型的精度并将其赋值给a0\n",
    "\n",
    "        # get the optimal attribution for a new branching\n",
    "        new_node.attr, div_value = OptAttr_Ent(df_train)\n",
    "        # get the new branch\n",
    "        if div_value == 0:  # categoric variable\n",
    "            value_count = ValueCount(data[new_node.attr])\n",
    "            for value in value_count:\n",
    "                df_v = df_train[df_train[new_node.attr].isin([value])]\n",
    "                df_v = df_v.drop(new_node.attr, 1)\n",
    "\n",
    "                # 先进行分支，得到分支后的模型（目的是为了计算分支后的模型精度）\n",
    "                new_node_child = Node(None, None, {})\n",
    "                label_arr_child = df_v[df_v.columns[-1]]\n",
    "                if len(df_v) == 0:\n",
    "                    new_node.attr_down[value] = Node(None,new_node.label, {})\n",
    "                else:\n",
    "                    label_count_child = NodeLabel(label_arr_child)\n",
    "                    new_node_child.label = max(label_count_child, key=label_count_child.get)\n",
    "                    new_node.attr_down[value] = new_node_child\n",
    "\n",
    "            # calculating to check whether need further branching\n",
    "            a1 = PredictAccuracy(new_node, df_test)  # 在测试集test上计算进行分支的模型的精度并将其赋值给a1\n",
    "            if a1 > a0:  # need branching                          #判断两者精度大小，如果分支后的模型精度更大则迭代生成子树\n",
    "                for value in value_count:\n",
    "                    df_v = df_train[df_train[new_node.attr].isin([value])]\n",
    "                    df_v = df_v.drop(new_node.attr, 1)\n",
    "                    new_node.attr_down[value] = PrePurn(df_v,df_test,a1)\n",
    "            else:  # 若不分枝的精度大，则不生成子树，并将该节点作为叶子节点（即剪枝）\n",
    "                new_node.attr = None\n",
    "                new_node.attr_down = {}\n",
    "\n",
    "        else:  # continuous variable # left and right child\n",
    "            value_l = \"<=%.3f\" % div_value\n",
    "            value_r = \">%.3f\" % div_value\n",
    "            df_v_l = df_train[df_train[new_node.attr] <= div_value]  # get sub set\n",
    "            df_v_r = df_train[df_train[new_node.attr] > div_value]\n",
    "\n",
    "            # for child node\n",
    "            new_node_l = Node(None, None, {})\n",
    "            new_node_r = Node(None, None, {})\n",
    "            label_count_l = NodeLabel(df_v_l[df_v_r.columns[-1]])\n",
    "            label_count_r = NodeLabel(df_v_r[df_v_r.columns[-1]])\n",
    "\n",
    "            # 先进行分支，得到分支后的模型，用于计算精度以比较\n",
    "            new_node_l.label = max(label_count_l, key=label_count_l.get)\n",
    "            new_node_r.label = max(label_count_r, key=label_count_r.get)\n",
    "            new_node.attr_down[value_l] = new_node_l\n",
    "            new_node.attr_down[value_r] = new_node_r\n",
    "\n",
    "            # calculating to check whether need further branching\n",
    "            a1 = PredictAccuracy(new_node, df_test)  # 计算分支后的模型精度比较并决定是否分支\n",
    "            if a1 > a0:  # need branching\n",
    "                new_node.attr_down[value_l] = PrePurn(df_v_l,df_test,a1)\n",
    "                new_node.attr_down[value_r] = PrePurn(df_v_r,df_test,a1)\n",
    "            else:\n",
    "                new_node.attr = None\n",
    "                new_node.attr_down = {}\n",
    "\n",
    "    return new_node\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （6）带后剪枝的树构建函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#后剪枝函数\n",
    "import pandas as pd\n",
    "data_file_encode = \"gb18030\"\n",
    "with open(\"watermelon_2.csv\", mode = 'r', encoding = data_file_encode) as data_file:\n",
    "    data = pd.read_csv(data_file)\n",
    "def PostPurn(root, df_test):\n",
    "    '''\n",
    "    预先 生成完整的决策树 \n",
    "    \n",
    "    @param root：节点，树的根 \n",
    "    @param df_test：数据框，用于判断是否进行剪枝的测试集 \n",
    "    @return 通过遍历树的准确度得分\n",
    "    '''\n",
    "    # 叶节点的预测精度\n",
    "    if root.attr == None:  \n",
    "        return PredictAccuracy(root, df_test)\n",
    "    \n",
    "    # 计算子节点的测试精度\n",
    "    a1 = 0\n",
    "    value_count = ValueCount(data[root.attr]) \n",
    "    for value in list(value_count):\n",
    "        df_test_v = df_test[ df_test[root.attr].isin([value]) ]  # 获取子集\n",
    "        if len(df_test_v) == 0:\n",
    "            continue\n",
    "        if value in root.attr_down:  # 当前节点存在属性\n",
    "            a1_v = PostPurn(root.attr_down[value], df_test_v) # 按属性进行划分子数据集和新的根节点递归\n",
    "        else:  # 当前节点不存在属性\n",
    "            a1_v = PredictAccuracy(root, df_test_v) # 计算精度\n",
    "        if a1_v == -1:  # -1 表示该节点不需要剪枝\n",
    "            return -1 # 该节点上一节点也不需要剪枝\n",
    "        else:\n",
    "            a1 += a1_v * len(df_test_v.index) / len(df_test.index) #对各子数据集按属性划分后各类的准确率进行求和\n",
    "             \n",
    "    # 计算剪枝后节点上的测试准确度\n",
    "    node = Node(None, root.label, {})\n",
    "    a0 = PredictAccuracy(node, df_test)\n",
    "    \n",
    "    # 检查是否需要修剪\n",
    "    if a0 > a1:\n",
    "        root.attr = None\n",
    "        root.attr_down = {}\n",
    "        return a0 # 返回该节点剪枝后测试集的精度\n",
    "    else: \n",
    "        return -1 # 该节点不需要剪枝\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.决策树可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化并保存为PNG\n",
    "def DrawPNG(root, out_file):\n",
    "    '''\n",
    "    visualization of decision tree from root.\n",
    "    @param root: Node, the root node for tree.\n",
    "    @param out_file: str, name and path of output file\n",
    "    '''\n",
    "    try:\n",
    "        from pydotplus import graphviz\n",
    "    except ImportError:\n",
    "        print(\"module pydotplus.graphviz not found\")\n",
    "        \n",
    "    g = graphviz.Dot()  # generation of new dot   \n",
    "\n",
    "    TreeToGraph(0, g, root)\n",
    "    g2 = graphviz.graph_from_dot_data( g.to_string() )\n",
    "    \n",
    "    g2.write_png(out_file) \n",
    "    \n",
    "#将结构类型转化为图\n",
    "def TreeToGraph(i, g, root):\n",
    "    '''\n",
    "    build a graph from root on\n",
    "    @param i: node number in this tree\n",
    "    @param g: pydotplus.graphviz.Dot() object\n",
    "    @param root: the root node\n",
    "    \n",
    "    @return i: node number after modified  \n",
    "#     @return g: pydotplus.graphviz.Dot() object after modified\n",
    "    @return g_node: the current root node in graphviz\n",
    "    '''\n",
    "    try:\n",
    "        from pydotplus import graphviz\n",
    "    except ImportError:\n",
    "        print(\"module pydotplus.graphviz not found\")\n",
    "\n",
    "    if root.attr == None:\n",
    "        g_node_label = \"Node:%d\\n好瓜:%s\" % (i, root.label)\n",
    "    else:\n",
    "        g_node_label = \"Node:%d\\n好瓜:%s\\n属性:%s\" % (i, root.label, root.attr)\n",
    "    g_node = i\n",
    "    g.add_node( graphviz.Node( g_node, label = g_node_label, fontname=\"FangSong\" ) )\n",
    "    \n",
    "    for value in list(root.attr_down):\n",
    "        i, g_child = TreeToGraph(i+1, g, root.attr_down[value])\n",
    "        g.add_edge( graphviz.Edge(g_node, g_child, label = value, fontname=\"FangSong\") ) \n",
    "\n",
    "    return i, g_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、由数据创建树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of full tree: 0.429\n",
      "accuracy of pre-purning tree: 0.714\n",
      "accuracy of post-purning tree: 0.714\n",
      "accuracy: 1.000  0.333  0.667  0.667  0.000  \n",
      "average accuracy: 0.533\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "\n",
    "'''''\n",
    "create on 2017/3/24, the day after our national football team beat south korea\n",
    "@author: PY131\n",
    "'''''\n",
    "\n",
    "'''\n",
    "import data and pre-analysis through data visualization\n",
    "'''\n",
    "# using pandas dataframe for .csv read which contains chinese char.\n",
    "import pandas as pd\n",
    "data_file_encode = \"gb18030\"\n",
    "with open(\"watermelon_2.csv\", mode = 'r', encoding = data_file_encode) as data_file:\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "# 训练集划分\n",
    "index_train = [0,1,2,5,6,9,13,14,15,16]\n",
    "\n",
    "df_train = df.iloc[index_train]\n",
    "df_test  = df.drop(index_train)\n",
    "\n",
    "# 生成一棵完整的树\n",
    "root = TreeGenerate(df_train)\n",
    "DrawPNG(root, \"decision_tree_full.png\")\n",
    "print(\"accuracy of full tree: %.3f\" % PredictAccuracy(root, df_test))\n",
    "\n",
    "# 预剪枝\n",
    "root = PrePurn(df_train, df_test)\n",
    "DrawPNG(root, \"decision_tree_pre.png\")\n",
    "print(\"accuracy of pre-purning tree: %.3f\" % PredictAccuracy(root, df_test))\n",
    "\n",
    "# 后剪枝\n",
    "root = TreeGenerate(df_train)\n",
    "PostPurn(root, df_test)\n",
    "DrawPNG(root, \"decision_tree_post.png\")\n",
    "print(\"accuracy of post-purning tree: %.3f\" % PredictAccuracy(root, df_test))\n",
    "\n",
    " ## 与ID-3代码一致\n",
    "# print the accuracy\n",
    "# k-folds cross prediction\n",
    "accuracy_scores = []\n",
    "n = len(df.index)\n",
    "k = 5\n",
    "for i in range(k):\n",
    "    m = int(n/k)\n",
    "    test = []\n",
    "    for j in range(i*m, i*m+m):\n",
    "        test.append(j)\n",
    "        \n",
    "    df_train = df.drop(test)\n",
    "    df_test = df.iloc[test]\n",
    "    root = TreeGenerate(df_train)  # generate the tree\n",
    "    PostPurn(root, df_test)  # post-purning\n",
    "    \n",
    "    # test the accuracy\n",
    "    pred_true = 0\n",
    "    for i in df_test.index:\n",
    "        label = Predict(root, df[df.index == i])\n",
    "        if label == df_test[df_test.columns[-1]][i]:\n",
    "            pred_true += 1\n",
    "            \n",
    "    accuracy = pred_true / len(df_test.index)\n",
    "    accuracy_scores.append(accuracy) \n",
    "\n",
    "# print the prediction accuracy result\n",
    "accuracy_sum = 0\n",
    "print(\"accuracy: \", end = \"\")\n",
    "for i in range(k):\n",
    "    print(\"%.3f  \" % accuracy_scores[i], end = \"\")\n",
    "    accuracy_sum += accuracy_scores[i]\n",
    "print(\"\\naverage accuracy: %.3f\" % (accuracy_sum/k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：本代码给出了两种决策树的两种构建函数（基尼指数（Cart决策树），信息增益                                                                                                                                      （ID4决策树））。但是本代码构建时实际采用的时基尼指数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不剪枝生成的树\n",
    "![](decision_tree_full.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预剪枝生成的树（出现欠拟合，效果很差）\n",
    "![](decision_tree_pre.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   后剪枝生成的树  \n",
    "\n",
    "![](decision_tree_post.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
